{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install TensorFlow and keras! You can do it by running the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bash: sor: 2: pip3: parancs nem tal치lhat칩\n",
      "bash: sor: 3: pip3: parancs nem tal치lhat칩\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "pip3 install --upgrade tensorflow\n",
    "pip3 install --upgrade keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get the following error:\n",
    "\n",
    "ImportError: cannot import name np_utils\n",
    "\n",
    "Then install np_utils package (pip3 install np_utils)\n",
    "\n",
    "If you are using python2 maybe you have to install the packages with pip!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to import the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "def accuracy(Y,P):\n",
    "    return np.count_nonzero(np.equal(np.argmax(Y,axis=-1),np.argmax(P,axis=-1)).astype(int))/float(Y.shape[0])\n",
    "\n",
    "def visualize(data):\n",
    "    p = data.reshape((28,28))\n",
    "    plt.imshow(p,interpolation='none', cmap='gray_r')\n",
    "    plt.show()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST dataset contains handwritten digit, it has 60000 train image and 10000 test image. It's a commonly used dataset for training and testing machine learning models, because it contains real word examples, it's not that big, so it's fast to try out learning algorithms and experiment on this. The dataset is a subset of a larger database collected by National Institute of Standards and Technology (NIST) in the 90's. The purpose of collecting the dataset to create a database which can be used to create algorithms to be used in automatic ZIP code reader systems. The numbers in the training set was taken from American Census Bureau:\n",
    "<img src=\"images/hwd_example.PNG\" width=\"300px\" />\n",
    "\n",
    "The test set was collected from American high school students. Because train and test data comes from very different sources  it's not well suited for machine learning experiments. This was the reason for creating the MNIST (the images also was normalized to fit into 28x28 pixel and transformed to grayscale). In 2017 a larger dataset (EMNIST) was created from the original data containing 240000 training images and 40000 testing images. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells below will load the data into a numpy matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 5s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape((x_train.shape[0], x_train.shape[1]*x_train.shape[2]))/255.\n",
    "x_test = x_test.reshape((x_test.shape[0], x_test.shape[1]*x_test.shape[2]))/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000,)\n",
      "[5 0 4 1 9 2 1 3 1 4]\n",
      "(10000, 784)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(y_train[0:10])\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the dataset have 60000 training images and 10000 test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nW3pCAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6860698190>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize(x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The label is simply the number you can see on the picture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "\n",
    "The goal of this notebook is to implement neural networks in numpy and train it with gradient descent to recognize handwritten digits. \n",
    "\n",
    "The notebook contains 2 homeworks: \n",
    "* implementing the forward step of an L-layer neural network (due 03.06)\n",
    "* implementing the backward step of an L-layer neural network, implementing the gradient descent and mini-batch gradient descent (due 03.13)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding\n",
    "\n",
    "The labels are discrete variables. To train a classifier we need to transform them to probabilities. This transformation is called one-hot encoding. It is defined as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "y\\in \\{0,1,2,...,K\\}\\longrightarrow y\\in \\{0,1\\}^K \\\\\n",
    "y=l \\Longrightarrow y = [0, 0, \\dots, 0]\\;(K \\mathrm{dimensional}),\\; y_l = 1\n",
    "\\end{equation}\n",
    "\n",
    "Implement this transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GRADED function\n",
    "#Don't change the function name, parameters and return values\n",
    "def one_hot(y):\n",
    "    \"\"\"\n",
    "        Transforms labels to one-hot encoded labels.\n",
    "        Input: \n",
    "            * y:   (M,)\n",
    "        Output:\n",
    "            * y_oh: (M, K)\n",
    "    \"\"\"\n",
    "    y_oh = None\n",
    "    ###Start code here\n",
    "    \n",
    "    # set y_oh to the one-hot encoded version of labels y\n",
    "    y_oh = np.zeros((len(y), max(y)+1))\n",
    "    for x in range(0, len(y)):\n",
    "        y_oh[x,y[x]] = 1\n",
    "        \n",
    "    ###End code here\n",
    "    return y_oh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot(y_train)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Expected result:</b>\n",
    "\\begin{equation}\n",
    "\\begin{bmatrix}\n",
    "0.&0.& 0.& 0.& 0.& 1.& 0.& 0.& 0.& 0.\\\\\n",
    "1.& 0.& 0.& 0.& 0.& 0.& 0.& 0.& 0.& 0.\\\\\n",
    "0.& 0.&0.& 0.& 1.& 0.& 0.& 0.& 0.& 0.\\\\\n",
    "0.& 1.& 0.& 0.& 0.& 0.& 0.& 0.& 0.& 0.\\\\\n",
    "0.& 0.& 0.& 0.& 0.& 0.& 0.& 0.& 0.& 1.\\\\\n",
    "0.& 0.& 1.& 0.& 0.& 0.& 0.& 0.& 0.& 0.\\\\\n",
    "0.& 1.& 0.& 0.& 0.& 0.& 0.& 0.& 0.& 0.\\\\\n",
    "0.& 0.& 0.& 1.& 0.& 0.& 0.& 0.& 0.& 0.\\\\\n",
    "0.& 1.& 0.& 0.& 0.& 0.& 0.& 0.& 0.& 0.\\\\\n",
    "0.& 0.& 0.& 0.& 1.& 0.& 0.& 0.& 0.& 0.\n",
    "      \\end{bmatrix}\n",
    "       \\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Transforming the whole dataset</b>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "y_train_oh = one_hot(y_train)\n",
    "y_test_oh = one_hot(y_test)\n",
    "\n",
    "print(y_train_oh.shape)\n",
    "print(y_test_oh.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L layer neural network\n",
    "\n",
    "\n",
    "Neural network can be described with the following computational graph:\n",
    "<img src=\"images/NeuralNetworkForwardPass.png\" />\n",
    "\n",
    "This graph describes the forward propagation in the neural network. As you can see, every linear unit needs a $W$ matrix and a $b$ vector. First we need to initialize the weights of the networks. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the weights of an L layer network\n",
    "\n",
    "As you can see above the computational graph also shows the dimensions of these objects. It's important to initialize the $W$\n",
    "matrices to be random. To see this, imagine the following network:\n",
    "\n",
    "<img src=\"images/simple_net.png\" />\n",
    "\n",
    "If we initialize the weights to zeros (or each neuron have the same weights), then the network will have the following symmetry: \n",
    "\\begin{equation}\n",
    "a^{[1]}_0=a^{[1]}_1\n",
    "\\end{equation}\n",
    "\n",
    "So the two neurons in the first layer computes exactly the same function in the first step. When we calculate the derivatives with backpropagation, the gradient's will also reflect this symmetry:\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial W^{[1]}_0}=\\frac{\\partial L}{\\partial W^{[1]}_1}\n",
    "\\end{equation}\n",
    "\n",
    "This means, that the weights of the first and second neuron will be the same for every iteration when we try to train the network with gradient descent ($W=W-\\alpha\\frac{\\partial L}{\\partial W}$). So in case of $n$ neuron, instead of $n$ neuron we will have just one, but we will do the single neuron computation $n$ times. To break this symmetry you need to initialize the $W$ matrices to be random. This will break the symmetry. The $b$ values can be set to zeros.\n",
    "\n",
    "<b>Takeaway message: </b> to break the symmetry (every neuron in the layer computes the same) you need to initialize the weights matrices ($W$) to be random!\n",
    "\n",
    "\n",
    "#### Multiplying with a small number\n",
    "\n",
    "\n",
    "We also want the randomly initialized weight matrix to have small values. If the values of the weight matrix aren't small, then the output of the neuron won't be small either. Because after the linear unit we go trough some non-linearity, and in case of a sigmoid non-linearity it means we are in the flat part of the function:\n",
    "<img src=\"images/sigmoid.PNG\" />\n",
    "\n",
    "If we are in the flat part as you can see in the plot, the derivative is pretty small. This means the change in the weight matrix will be very small, so the training will slow down. To avoid this, we multiply the weights with a small number, let's say with $0.01$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GRADED function\n",
    "#Don't change the function name, parameters and return values\n",
    "def initialize_weights(layer_dimensions):\n",
    "    \"\"\"\n",
    "        Initialize the weights of the neural network.\n",
    "        Input:\n",
    "            * layer_dimensions: list containing the number of neurons for each layer. \n",
    "                                layer_dimensions[0] is the dimensionality of our data. \n",
    "                                layer_dimensions[1] is the number of neurons in the first layer.\n",
    "        Output:\n",
    "            * parameters: dict containing the weights of the network.\n",
    "                          parameters[\"W1\"] = the W weight matrix of the first layer\n",
    "                          parameters[\"b1\"] = the b bias node of the first layer\n",
    "    \"\"\"\n",
    "    number_of_layers = len(layer_dimensions) - 1\n",
    "    parameters={}\n",
    "    ###Start code here\n",
    "    np.random.seed(0)\n",
    "    W_string = \"W\"\n",
    "    b_string = \"b\"\n",
    "    for x in range(0, number_of_layers):\n",
    "     #   print W_string+str(x)\n",
    "        \n",
    "        Ws = (np.random.randn(layer_dimensions[x+1],layer_dimensions[x]))*0.01\n",
    "        Bs = np.zeros([layer_dimensions[x+1],1])\n",
    "        parameters[W_string+str(x+1)] = Ws  \n",
    "        parameters[b_string+str(x+1)] = Bs  \n",
    "    # initialize the weights matrix W with small random numbers (multiply by 0.01), and b to zeros FOR ALL LAYERS!\n",
    "\n",
    "    ###End code here\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1.shape= (6, 3)\n",
      "b1.shape= (6, 1)\n",
      "W1= [[ 0.01764052  0.00400157  0.00978738]\n",
      " [ 0.02240893  0.01867558 -0.00977278]\n",
      " [ 0.00950088 -0.00151357 -0.00103219]\n",
      " [ 0.00410599  0.00144044  0.01454274]\n",
      " [ 0.00761038  0.00121675  0.00443863]\n",
      " [ 0.00333674  0.01494079 -0.00205158]]\n",
      "b1= [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "parameters = initialize_weights([3,6])\n",
    "print(\"W1.shape=\",parameters[\"W1\"].shape)\n",
    "print(\"b1.shape=\",parameters[\"b1\"].shape)\n",
    "print(\"W1=\",parameters[\"W1\"])\n",
    "print(\"b1=\",parameters[\"b1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Expected outpt:</b>\n",
    "\n",
    "W1.shape= (6, 3)\n",
    "\n",
    "b1.shape= (6, 1)\n",
    "\n",
    "W1=\\begin{equation}\\begin{bmatrix} 0.01764052&  0.00400157& 0.00978738\\\\\n",
    "  0.02240893&  0.01867558 &-0.00977278]\\\\\n",
    "  0.00950088& -0.00151357& -0.00103219]\\\\\n",
    "  0.00410599&  0.00144044 & 0.01454274]\\\\\n",
    "  0.00761038 & 0.00121675&  0.00443863]\\\\\n",
    "  0.00333674 & 0.01494079 &-0.00205158]]\n",
    " \\end{bmatrix}\n",
    " \\end{equation}\n",
    " \n",
    "b1= \\begin{equation}\n",
    "\\begin{bmatrix}0\\\\\n",
    " 0.\\\\\n",
    " 0.\\\\\n",
    " 0.\\\\\n",
    " 0.\\\\\n",
    " 0.\n",
    " \\end{bmatrix}\n",
    " \\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward propagation in the network\n",
    "\n",
    "\n",
    "To evaluate the network and make prediction, calculate the loss function you have to go trough the computational graph shown above. As you can see the graph contains modules. To implement the whole calculation you need to implement these simple modules, step by step.\n",
    "\n",
    "\n",
    "### Linear step\n",
    "\n",
    "\n",
    "The first module is the linear unit. The calculation is illustrated in the following graph:\n",
    "<img src=\"images/nn_linear.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GRADED function\n",
    "#Don't change the function name, parameters and return values\n",
    "def linear_forward(W, b, a_prev):\n",
    "    \"\"\"\n",
    "        Linear unit in the computational graph.\n",
    "        Inputs:\n",
    "                * W: the weight matrix of the unit (dimensions in the graph above)\n",
    "                * b: the bias vector of the unit (dimensions in the graph above)\n",
    "                * a_prev: the activation from the previous node (dimensions in the graph above)\n",
    "        Outputs:\n",
    "                * Z:     the result (dimensions in the graph above)\n",
    "                * cache: combination of values you will need in backward passes\n",
    "    \"\"\"\n",
    "    ###Start code here\n",
    "    \n",
    "    # Calculate Z\n",
    "    Z = None\n",
    "    Z = np.subtract(W.dot(a_prev), b) \n",
    "    \n",
    "    # Create a cache for backpropagation, suggestion: (Z, W, a_prev)\n",
    "    cache = None\n",
    "    cache = (Z, W, a_prev)\n",
    "    ###End code here\n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01514415]\n",
      " [0.01947252]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "parameters = initialize_weights([3,2])\n",
    "x = np.random.randn(1,3).T\n",
    "Z, cache = linear_forward(parameters[\"W1\"],parameters[\"b1\"],x)\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Expected output</b>\n",
    "\\begin{equation}\n",
    "\\begin{bmatrix}0.01514415\\\\\n",
    " 0.01947252\n",
    "    \\end{bmatrix}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-linear step\n",
    "\n",
    "After we calculated the linear node, the result of this will go trough some non-linearity. This is shown in the graph below:\n",
    "\n",
    "<img src=\"images/nn_nonlinear.png\" />\n",
    "\n",
    "We choose the $g$ function to be sigmoid function in this case. Implement this function. The sigmoid function:\n",
    "$f(x)=\\frac{1}{1+e^{-x}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GRADED function\n",
    "#Don't change the function name, parameters and return values\n",
    "def activation_forward(Z):\n",
    "    \"\"\"\n",
    "        Activation node on the forward computational graph.\n",
    "        Inputs:\n",
    "                Z: the ouput of linear unit\n",
    "        Outputs:\n",
    "                a: the activation\n",
    "    \"\"\"\n",
    "    ###Start code here\n",
    "   \n",
    "    #implement sigmoid activation\n",
    "    a = None\n",
    "    a = 1 / (1 + np.exp(-Z))\n",
    "\n",
    "    ###End code here\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.85371646, 0.59872543, 0.72685773],\n",
       "       [0.9038621 , 0.86617546, 0.27343225]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "Z = np.random.randn(2,3)\n",
    "activation_forward(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Expected output:</b>\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{bmatrix}\n",
    "0.85371646& 0.59872543& 0.72685773\\\\\n",
    "       0.9038621 & 0.86617546& 0.27343225\n",
    "       \\end{bmatrix}\n",
    "       \\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax step\n",
    "\n",
    "The last part of the computational graph is a softmax unit.\n",
    "\n",
    "<img src=\"images/nn_softmax.png\" />\n",
    "\n",
    "Implement the function shown in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GRADED function\n",
    "#Don't change the function name, parameters and return values\n",
    "def softmax(Z):\n",
    "    \"\"\"\n",
    "        Calculates the softmax of a matrix.\n",
    "        Input: \n",
    "            * Z: (M,K)=(num_examples, num_categories)\n",
    "        Returns:\n",
    "            * a: (M,K)=(num_examples, num_categories)\n",
    "    \"\"\"\n",
    "    ###Start code here\n",
    "    \n",
    "    # Transform Z to probabilites\n",
    "    # The sum should be on categories!\n",
    "    a = None\n",
    "    #print Z\n",
    "    exponent = np.exp(Z)    \n",
    "    a = (exponent.T / np.sum(exponent,axis = 1)).T\n",
    "    ###End code here\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.58423523, 0.14936733, 0.26639744],\n",
       "       [0.57854881, 0.39829292, 0.02315827]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "Z = np.random.randn(2,3)\n",
    "softmax(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Expected output:</b>\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{bmatrix}\n",
    "0.58423523& 0.14936733& 0.26639744\\\\\n",
    "0.57854881& 0.39829292& 0.02315827\n",
    "\\end{bmatrix}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full network forward step\n",
    "\n",
    "Congratulations, you implemented all the units of a the computational graph. Now your job is to implement the full network using the units you implemented! So the computation you have to do can be shown with the following computational graph:\n",
    "\n",
    "<img src=\"images/nn_forward.png\" />\n",
    "\n",
    "Implement this calculation!\n",
    "\n",
    "<b>REMEMBER: use the units you implemented above (function calls).<b/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GRADED function\n",
    "#Don't change the function name, parameters and return values\n",
    "def nn_forward(parameters, X):\n",
    "    \"\"\"\n",
    "        Calculates a forward step in the network.\n",
    "        Input:\n",
    "            * parameters: dict of parameters.\n",
    "                          parameters[\"W1\"]= the W matrix of first layer\n",
    "                          parameters[\"b4\"]=the bias vector in layer 4\n",
    "            * X: input matrix. Shape (M,K)\n",
    "    \"\"\"\n",
    "    caches = []\n",
    "    L = len(parameters)//2\n",
    "    ###Start code here\n",
    "    \n",
    "    # Set a_prev to the input!\n",
    "    # In the first node of the computational graph, the input is transposed!\n",
    "    a = X.T\n",
    "    W_string = \"W\"\n",
    "    b_string = \"b\"\n",
    "    for x in range(0, L):\n",
    "     #   print W_string+str(x)\n",
    "        W = parameters[W_string+str(x+1)]\n",
    "        b = parameters[b_string+str(x+1)]\n",
    "        Z, cache = linear_forward(W, b, a)\n",
    "        caches.append(cache)\n",
    "        a = activation_forward(Z)\n",
    "        \n",
    "    # Go trough the nodes of the computational graph, from LEFT to RIGHT\n",
    "    # Always use as the input of current node the ouput of previous node!\n",
    "    # Get the W, b weights from the parameters dictionary\n",
    "    # use linear_forward and activation_forward function to step one in the layer\n",
    "    # append the linear unit's cache to caches list\n",
    "    \n",
    "    \n",
    "    # Important: the last layer activation should be softmax! \n",
    "    # Important: Transpose Z (in the last unit)\n",
    "    #\n",
    "    a = softmax(a)\n",
    "  \n",
    "    ###End code here\n",
    "    \n",
    "    return (a,caches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.49999642 0.50000358]\n",
      " [0.49999529 0.50000471]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "parameters = initialize_weights([3,4,2])\n",
    "x = np.random.randn(2,3)\n",
    "AL, cache = nn_forward(parameters, x)\n",
    "print(AL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Expected output:</b>\n",
    "\\begin{equation}\n",
    "\\begin{bmatrix}\n",
    "0.49967587& 0.50032413\\\\\n",
    "0.49966831& 0.50033169\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "HOMEWORK 1 END\n",
    "<hr style=\"height:3px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "\n",
    "\n",
    "To calculate the loss function you have to implement the computation in the following graph:\n",
    "<img src=\"images/nn_loss.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GRADED function\n",
    "#Don't change the function name, parameters and return values\n",
    "def crossentropy(P, Y):\n",
    "    \"\"\"\n",
    "        Calculates the crossentropy-loss shown in the computational graph above.\n",
    "        Inputs:\n",
    "                * P the prediction of the network (dimensions above)\n",
    "                * Y the one-hot encoded labels\n",
    "        Outputs:\n",
    "                * The crossentropy between the P and Y\n",
    "    \"\"\"\n",
    "    ### Start code here\n",
    "   \n",
    "    #calculate the coross entropy loss\n",
    "    loss = None\n",
    "    Dim_P_0 = P.shape[0]\n",
    "    Dim_P_1 = P.shape[1]\n",
    "    \n",
    "    Dim_Y_0 = Y.shape[0]\n",
    "    Dim_Y_1 = Y.shape[1]\n",
    "    \n",
    "    loss = np.sum(np.multiply(Y,(np.log(P))))\n",
    "    loss = - 1 * loss / Dim_Y_0\n",
    "    ### End code here\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6931483062762567"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "parameters = initialize_weights([3,4,2])\n",
    "x = np.random.randn(2,3)\n",
    "AL, cache = nn_forward(parameters, x)\n",
    "y = np.array(([[0,1],[1,0]]))\n",
    "crossentropy(AL,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Expected output:</b>\n",
    "\n",
    "0.6931397896985225"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward propagation in the network\n",
    "\n",
    "\n",
    "Congratulations! You implemented a whole L layer neural network for classification problems! \n",
    "\n",
    "In the next step you will implement backpropagation to be able to train the network.\n",
    "\n",
    "In the forward propagation you go trough a computational graph from left to right. In the backpropagation you go trough a computational graph from right to left. \n",
    "\n",
    "The computational graph of the backpropagation in $L$ layer neural network is the following.\n",
    "\n",
    "<img src=\"images/NeuralNetworkBackwardPassV3.png\" />\n",
    "\n",
    "To implement this, you will firs implement the different units this calculation needs.\n",
    "\n",
    "### Backpropagation trough non-linearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss and softmax step (last node on the graph)\n",
    "\n",
    "\n",
    "Because we go trough on the graph from the right to the left the first node is the $\\frac{\\partial L}{\\partial Z^{[L]}}$ unit.  When we use softmax activation and cross entropy functions the derivate will be simple in the top of the network. Implement this calculation as shown in the computational graph:\n",
    "<img src=\"images/nn_softloss_backward.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GRADED function\n",
    "#Don't change the function name, parameters and return values\n",
    "def losssoftmax_backward(P,Y):\n",
    "    \"\"\"\n",
    "        Calculates the dLdZ derivate which is in the top of the network. \n",
    "        Inputs:\n",
    "                * P the prediction of the network\n",
    "                * Y the labels\n",
    "        Outputs:\n",
    "                * dLdZ the derivate of the L loss function \n",
    "    \"\"\"\n",
    "    ###Start code here\n",
    "    \n",
    "    #implement the calculation shown in the graph\n",
    "    dLdZ = None\n",
    "    \n",
    "    Dim_P_0 = P.shape[0]\n",
    "    Dim_P_1 = P.shape[1]\n",
    "    \n",
    "    Dim_Y_0 = Y.shape[0]\n",
    "    Dim_Y_1 = Y.shape[1]\n",
    "    \n",
    "    dLdZ = np.subtract(P,Y.T)\n",
    "    dLdZ = dLdZ / Dim_Y_0\n",
    "    \n",
    "    ###End code here\n",
    "    return dLdZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.11110921, -0.22222253, -0.22222001],\n",
       "       [-0.22222425,  0.11111118,  0.11111307]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "parameters = initialize_weights([3,4,2])\n",
    "x = np.random.randn(3,3)\n",
    "AL, cache = nn_forward(parameters, x)\n",
    "y = np.array(([[0,1],[1,0],[1,0]]))\n",
    "\n",
    "losssoftmax_backward(AL,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Expected output:</b>\n",
    "\\begin{equation}\n",
    "\\begin{bmatrix}\n",
    "0.1665686 & -0.16676222& -0.16677319\\\\\n",
    "       -0.1665686&  0.16676222&  0.16677319\n",
    "\\end{bmatrix}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General non-linear backward step in the network\n",
    "\n",
    "Implement a backward step on non-linearity in the model.  This computation is shown in the following graph:\n",
    "\n",
    "<img src=\"images/nn_nonlin_backward.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GRADED function\n",
    "#Don't change the function name, parameters and return values\n",
    "def activation_backward(dLda, cache):\n",
    "    \"\"\"\n",
    "        Calculates the backward step in a non-linearity.\n",
    "        Inputs:\n",
    "            * dLda: The derivates above the current unit.\n",
    "            * cache: The cache from the forward step.\n",
    "    \"\"\"\n",
    "    \n",
    "    ###Start code here\n",
    "    \n",
    "    # Use the cache from the foward step\n",
    "    # Calculate the g'(Z): the sigmoid derivate (the derivate is: sigmoid(Z)*(1-sigmoid(Z)) )\n",
    "    # Do the calculation shown in the graph.\n",
    "    dLdZ = None\n",
    "    Z = cache[0]\n",
    "    sigmoid = 1 / (1 + np.exp(-Z))\n",
    "    #print(\"Z:\",Z.shape,(sigmoid * (1 - sigmoid)).shape,np.multiply(sigmoid,1-sigmoid).shape)\n",
    "\n",
    "    dLdZ = np.multiply(dLda, np.multiply(sigmoid,1-sigmoid))    \n",
    "    ###End code here\n",
    "    return dLdZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01689878, -0.2133506 ],\n",
       "       [-0.06251018,  0.10071241],\n",
       "       [-0.14419048, -0.03281763],\n",
       "       [-0.08698937, -0.18342292]])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "Z      = np.random.randn(4,2)\n",
    "W      = np.random.randn(4,3)\n",
    "a_prev = np.random.randn(3,2)\n",
    "dLda   = np.random.randn(4,2)\n",
    "    \n",
    "\n",
    "dLdZ = activation_backward(dLda, (Z, W, a_prev))\n",
    "\n",
    "dLdZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Expected output:</b>\n",
    "\\begin{equation}\n",
    "\\begin{bmatrix}\n",
    "-0.01689878&-0.2133506 \\\\\n",
    "       -0.06251018& 0.10071241\\\\\n",
    "       -0.14419048& -0.03281763\\\\\n",
    "       -0.08698937& -0.18342292\n",
    "\\end{bmatrix}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation trough linear unit\n",
    "\n",
    "\n",
    "Implement the backpropagation trough a linear unit. This calculation is shown as the following graph:\n",
    "\n",
    "<img src=\"images/nn_linear_backward.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GRADED function\n",
    "#Don't change the function name, parameters and return values\n",
    "def linear_backward(dLdZ, cache):\n",
    "    \"\"\"\n",
    "        Implements the backward step on linear unit as shown in the graph above/\n",
    "        Inputs: \n",
    "                * dLdZ:  the derivate from the next node\n",
    "                * cache: the cache form froward step\n",
    "        Returns:\n",
    "                * dLda\n",
    "                * dLdW\n",
    "                * dLdb\n",
    "    \"\"\"\n",
    "    \n",
    "    ###Start code here\n",
    "    \n",
    "    # Use the cache from the forward step\n",
    "    Z = cache[0]\n",
    "    W = cache[1]\n",
    "    a_prev = cache[2]\n",
    "    \n",
    "    #Calculate dLda as shown in the graph\n",
    "    dLda = None\n",
    "    dLda = W.T.dot(dLdZ)\n",
    "    #Calculate the dLdW shown in the graph\n",
    "    dLdW = None\n",
    "  \n",
    "    dLdW = np.matmul(dLdZ,a_prev.T)\n",
    "    print(dLdW)\n",
    "    print(dLdZ)\n",
    "    print(a_prev)  \n",
    "    #Calculate the dLdb shown in the graph\n",
    "    # Use keepdims=True\n",
    "    dLdb = None\n",
    "    dLdb = np.sum(dLdZ, axis = 1)\n",
    "    \n",
    "    ###End code here\n",
    "    return (dLda, dLdW, dLdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.68215639e-01 -1.67937153e-01]\n",
      " [ 1.15796302e-04 -3.20954510e-04]]\n",
      "[[ 0.11111098 -0.22222276 -0.22222155]\n",
      " [-0.2222221   0.11111086  0.11111124]]\n",
      "[[0.50395216 0.50179708 0.50714939]\n",
      " [0.5057372  0.50193876 0.50664704]]\n",
      "dLda= [[ 0.00128503 -0.002226   -0.00222599]\n",
      " [-0.00108062  0.00079257  0.00079257]]\n",
      "dLdW= [[-1.68215639e-01 -1.67937153e-01]\n",
      " [ 1.15796302e-04 -3.20954510e-04]]\n",
      "dLdb= [-0.33333333  0.        ]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "parameters = initialize_weights([3,2,2])\n",
    "x = np.random.randn(3,3)\n",
    "AL, cache = nn_forward(parameters, x)\n",
    "y = np.array(([[0,1],[1,0],[1,0]]))\n",
    "\n",
    "dLdZ=losssoftmax_backward(AL,y)\n",
    "\n",
    "dLda, dLdW, dLdb = linear_backward(dLdZ, cache[-1])\n",
    "print(\"dLda=\",dLda)\n",
    "print(\"dLdW=\",dLdW)\n",
    "print(\"dLdb=\",dLdb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Expected output</b>\n",
    "\n",
    " \\begin{equation}dLda= \\begin{bmatrix}0.00238152&-0.00237077& -0.00237088\\\\\n",
    " -0.00085494 & 0.00085108 & 0.00085112\n",
    " \\end{bmatrix}\n",
    " \\end{equation}\n",
    " \\begin{equation}dLdW=\\begin{bmatrix}-0.08456424&-0.08032504\\\\\n",
    " 0.08456424 & 0.08032504\\\\\n",
    " \\end{bmatrix}\\end{equation}\n",
    " \\begin{equation}dLdb=\\begin{bmatrix}-0.16553337 \\\\ 0.16553337\\end{bmatrix}\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation trough the network\n",
    "\n",
    "\n",
    "Using the units you already implemented, implement the full backward pass trough the network as shown in the following graph:\n",
    "\n",
    "<img src=\"images/NeuralNetworkBackwardPassV3.png\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GRADED function\n",
    "#Don't change the function name, parameters and return values\n",
    "def nn_backward(X, Y, P, caches):\n",
    "    \"\"\"\n",
    "        Calculates the backward pass for the network as shown in the computational graph above.\n",
    "        Inputs:\n",
    "                * X: The input examples: (M,N) = (number_of_examples, dimensionality_of_data)\n",
    "                * Y: The one-hot encoded labels: (M,K) = (number_of_examples, number_of_categories)\n",
    "                * P: The network prediction: (M,K)\n",
    "                * caches: the cache from forward step\n",
    "        Returns:\n",
    "                * derivates: dictionary containing all the derivates\n",
    "                             derivates[\"dLdW2\"] = the derivate of the loss L regarding to the weight matrix W of 2. layer\n",
    "    \"\"\"\n",
    "    derivates = {}\n",
    "    \n",
    "    \n",
    "    ###Start code here\n",
    "    dLda_string = \"dLda\"\n",
    "    dLdW_string = \"dLdW\"\n",
    "    dLdb_string = \"dLdb\"\n",
    "    dLdZ_string = \"dLdZ\"\n",
    "    \n",
    "    dLdZ = losssoftmax_backward(P,Y)\n",
    "    for x in range(0, len(caches)):\n",
    "        # Linear backward\n",
    "        # Activation backward\n",
    "        print(len(caches)-x-1)\n",
    "        cache = caches[len(caches)-x-1] #itt...\n",
    "        \n",
    "        dLda, dLdW, dLdb = linear_backward(dLdZ, cache)\n",
    "        dLdZ = activation_backward(dLda, cache)\n",
    "        derivates[dLda_string+str(x+1)] = dLda\n",
    "        derivates[dLdW_string+str(x+1)] = dLdW\n",
    "        derivates[dLdb_string+str(x+1)] = dLdb\n",
    "        derivates[dLdZ_string+str(x+1)] = dLdZ\n",
    "    \n",
    "    # Implement the calculation shown in the graph!\n",
    "    # The arrows in the graph: the output of a unit is the input of the previous unit\n",
    "    # IMPORTANT: you have to go trough the graph from RIGHT to LEFT\n",
    "    \n",
    "    # The last node derivative (loss) initializes the calculation by setting the derivative in the top of the network\n",
    "    \n",
    "    \n",
    "    # compute the backward pass on non-linearity (use the function you implemented)\n",
    "    # compute the backward pass on linear unit (use the function you implemented)\n",
    "    # save the derivates to the derivates dict\n",
    "    # derivates[\"dLdW2\"] = the derivate of the loss L regarding to the weight matrix W of 2. layer\n",
    "        \n",
    " \n",
    "        \n",
    "    ###End code here\n",
    "    return derivates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[[-0.24974591 -0.24759381]\n",
      " [ 0.0016914   0.00432519]]\n",
      "[[-0.18749988 -0.18750011  0.06250058 -0.1875006 ]\n",
      " [ 0.0625004   0.06250019  0.06250041 -0.187501  ]]\n",
      "[[0.50395216 0.50179708 0.50714939 0.49527878]\n",
      " [0.5057372  0.50193876 0.50664704 0.48170677]]\n",
      "0\n",
      "[[ 4.14128951e-04  1.10360157e-04 -7.23228080e-04]\n",
      " [ 2.00209791e-04  5.58376980e-04  8.11545433e-05]]\n",
      "[[-4.61479833e-04 -4.61480340e-04  1.32324092e-04 -3.96969695e-04]\n",
      " [ 1.35104990e-04  1.35104864e-04  4.05066278e-05 -1.21520093e-04]]\n",
      "[[ 0.14404357  0.12167502  1.49407907 -0.85409574]\n",
      " [ 1.45427351  0.44386323 -0.20515826 -2.55298982]\n",
      " [ 0.76103773  0.33367433  0.3130677   0.6536186 ]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (3,4) (2,4) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-141-6eb95f301c47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mAL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mnn_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-106-e4f1dba360f7>\u001b[0m in \u001b[0;36mnn_backward\u001b[0;34m(X, Y, P, caches)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mdLda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdLdW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdLdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdLdZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mdLdZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivation_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdLda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mderivates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdLda_string\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdLda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mderivates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdLdW_string\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdLdW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-138-44f9443799c2>\u001b[0m in \u001b[0;36mactivation_backward\u001b[0;34m(dLda, cache)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m#print(\"Z:\",Z.shape,(sigmoid * (1 - sigmoid)).shape,np.multiply(sigmoid,1-sigmoid).shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mdLdZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdLda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;31m###End code here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdLdZ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (3,4) (2,4) "
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "parameters = initialize_weights([3,2,2])\n",
    "x = np.random.randn(4,3)\n",
    "y = (np.random.randn(4,2)>0.5).astype(int)\n",
    "AL, caches = nn_forward(parameters, x)\n",
    "\n",
    "nn_backward(x,y, AL, caches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "dLdW1 = \\begin{bmatrix}\n",
    "    -6.21632506e-04& -2.36318550e-04&  1.23490272e-03\\\\\n",
    "     3.61013616e-04& -1.50058030e-05& -4.73913042e-04\n",
    "     \\end{bmatrix}\n",
    "\\end{equation}\n",
    "     \\begin{equation}\n",
    "     dLdW2=\\begin{bmatrix}\n",
    "  0.00113032&  0.00250435\\\\\n",
    "       0.00110121& -0.00371535\n",
    "       \\end{bmatrix}\n",
    "       \\end{equation}\n",
    " \\begin{equation}\n",
    " dLdb1=\\begin{bmatrix}4.25433282e-06& -1.48633758e-06\n",
    " \\end{bmatrix}\\end{equation}\n",
    " \\begin{equation}\n",
    " dLdb2=\\begin{bmatrix} 0.00112994&-0.00112994\\end{bmatrix}\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the network\n",
    "\n",
    "Congratulation! You implemented the forward and backward calculation of a neural network. Training a network now it's very easy.\n",
    "\n",
    "### Gradient descent\n",
    "\n",
    "First, let's implement gradient descent for neural networks. The gradient descent finds the weights of the network using the following algorithm:\n",
    "\n",
    "```pseudo\n",
    "for every W,b:\n",
    "    W = random(W_shape)\n",
    "    b = random(b_shape)\n",
    "repeat{\n",
    "    for every W,b:\n",
    "        dLdW, dLdb = get_derivatives(X,Y, W, b)\n",
    "        W = W - alpha * dLdW\n",
    "        b = b - alpha * dLdb\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#GRADED function\n",
    "#Don't change the function name, parameters and return values\n",
    "def nn_train_gd(X, Y, layers, n_iter=1000, lr=0.01):\n",
    "    \"\"\"\n",
    "        Creates and trains a neural network with gradient descent.\n",
    "        Inputs:\n",
    "                * X: the images (number_of_examples, 28*28)\n",
    "                * Y: the labels (number_of_examples, 10)\n",
    "                * layers: the definition of neural networks\n",
    "                          [28*28, 100, 10]: two layers, 100 neuron in first layer, and 10 neurons in the last layer\n",
    "                          the first element of this list must be 28*28\n",
    "                          the number of neurons in the last layer must be the same as Y.shape[1]\n",
    "                * n_iter: how many iteration we want with gradient descent\n",
    "                * lr:     learning rate\n",
    "        Returns:\n",
    "                * parameters: the trained parameters of the network\n",
    "                * losses:     the loss values\n",
    "    \"\"\"\n",
    "    L = len(layers)\n",
    "    losses = []\n",
    "    \n",
    "    ###Start code here\n",
    "    \n",
    "    #initialize the weights of the network\n",
    "    parameters = None\n",
    "    \n",
    "    # In one iteration of gradient descent:\n",
    "      # Do a forward step in the network. Use nn_forward\n",
    "      # Calculate the derivates doing a backward step in the network. Use nn_backward\n",
    "      # Update the weights\n",
    "      # Calculate the loss. Use crossentropy. (and save it to losses)\n",
    "\n",
    "    \n",
    "    ###End code here\n",
    "    return parameters,losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This will take a while\n",
    "parameters,losses = nn_train_gd(x_train, y_train_oh, [28*28, 100, 10], n_iter=3, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "P, _ = nn_forward(parameters, x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy(y_train_oh, P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-batch gradient descent\n",
    "\n",
    "As you can see above, taking one step with gradient descent takes a lot of time, even with this smaller dataset (60k images). In deep learning instead of gradient descent we use mini-batch gradient descent or some more modern version of it. \n",
    "\n",
    "In mini-batch gradient descent we don't use the whole dataset in every iteration, instead we use a smaller sample (mini-batch). We iterate trough the dataset with these mini-batches, calculate the derivatives using these mini-batches and do a gradient descent step. After we went trough the dataset (we completed one epoch) we repeat the process for some epochs.\n",
    "\n",
    "So the mini-batch gradient descent algorithm is the following:\n",
    "```pseudo\n",
    "for every W,b:\n",
    "    W = random(W_shape)\n",
    "    b = random(b_shape)\n",
    "repeat{\n",
    "    for mini_batch_X, mini_batch_Y from X,Y:\n",
    "        for every W,b:\n",
    "            dLdW, dLdb = get_derivatives(mini_batch_X, mini_batch_Y, W,b)\n",
    "            W = W - alpha * dLdW\n",
    "            b = b - alpha * dLdb\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#GRADED function\n",
    "#Don't change the function name, parameters and return values\n",
    "def nn_train_sgd(X, Y, layers, batch_size=100, epochs=100, lr=0.01):\n",
    "    \"\"\"\n",
    "        Creates and trains a neural network with gradient descent.\n",
    "        Inputs:\n",
    "                * X: the images (number_of_examples, 28*28)\n",
    "                * Y: the labels (number_of_examples, 10)\n",
    "                * layers: the definition of neural networks\n",
    "                          [28*28, 100, 10]: two layers, 100 neuron in first layer, and 10 neurons in the last layer\n",
    "                          the first element of this list must be 28*28\n",
    "                          the number of neurons in the last layer must be the same as Y.shape[1]\n",
    "                * n_iter: how many iteration we want\n",
    "                * lr:     learning rate\n",
    "        Returns:\n",
    "                * parameters: the trained parameters of the network\n",
    "                * losses:     the loss values\n",
    "    \"\"\"\n",
    "   \n",
    "    losses = []\n",
    "    \n",
    "    ###Start code here\n",
    "    \n",
    "    #initialize the weights of the network\n",
    "    parameters = None\n",
    "    \n",
    "    # Iterate trough epochs\n",
    "       # Iterate trough batches in dataset\n",
    "          # Do a forward step in the network. Just with the mini-batch! Use nn_forward\n",
    "          # Calculate the derivates doing a backward step in the network. Just for the mini-batch! Use nn_backward\n",
    "          # Update the weights\n",
    "       # Calculate the loss. Use crossentropy. Save to losses.\n",
    "\n",
    "\n",
    "\n",
    "    ###End code here\n",
    "    return parameters,losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters,losses = nn_train_sgd(x_train, y_train_oh, [28*28, 100, 10], batch_size=100, epochs=20, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Expected output</b>\n",
    "\n",
    "Decreasing loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "P, _ = nn_forward(parameters, x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy(y_train_oh, P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Expected accuracy:</b> around 0.88 (88%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test accuracy\n",
    "\n",
    "To measure the performance of the algorithm we use a test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "P, _ = nn_forward(parameters, x_test)\n",
    "accuracy(y_test_oh, P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i=6\n",
    "visualize(x_test[i])\n",
    "print(\"Network predicts: \", np.argmax(P[i,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
